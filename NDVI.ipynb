{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123111222/GEE_Cloud-computing/blob/main/NDVI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A7zTTV3k7el8",
        "outputId": "3a72ae45-2afb-42f1-c44f-07e825cfe72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 16:26:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 13.248.244.96, 35.71.179.82, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-11 16:26:40 (89.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvLwSYoET9VO",
        "outputId": "75fa4671-6559-4b80-e8d7-a5263100d60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2t9Ll3NR7Mi4cXBkxn1Ol2J6LPM_6S2rnJksz8yzLA9Hrg3aA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3C_MvNXpOpkz",
        "outputId": "6ac5c244-3c58-416f-9e91-1b39998fb586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0eB-sT807tGs",
        "outputId": "7f438aec-6c72-4f8e-be1c-803f463bd665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NDVI/NDVI/code\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/NDVI/NDVI/code\"\n",
        "os.chdir(path)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H7ZZiKn2NJay",
        "outputId": "56e4448c-a969-4f51-f186-6a4113e8f1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-4.0.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Downloading patool-4.0.1-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-4.0.1\n",
            "Collecting sktime\n",
            "  Downloading sktime-0.37.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (24.2)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.2)\n",
            "Collecting scikit-base<0.13.0,>=0.6.1 (from sktime)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n",
            "Downloading sktime-0.37.0-py3-none-any.whl (37.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.0/37.0 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, sktime\n",
            "Successfully installed scikit-base-0.12.2 sktime-0.37.0\n",
            "Collecting reformer-pytorch==1.4.4\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch==1.4.4)\n",
            "  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch==1.4.4) (0.8.1)\n",
            "Collecting local-attention (from reformer-pytorch==1.4.4)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting product-key-memory (from reformer-pytorch==1.4.4)\n",
            "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch==1.4.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->reformer-pytorch==1.4.4)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch==1.4.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->reformer-pytorch==1.4.4) (1.3.0)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention->reformer-pytorch==1.4.4)\n",
            "  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch==1.4.4)\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from colt5-attention>=0.10.14->product-key-memory->reformer-pytorch==1.4.4) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->reformer-pytorch==1.4.4) (3.0.2)\n",
            "Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Downloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
            "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, reformer-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed axial-positional-embedding-0.3.12 colt5-attention-0.11.1 hyper-connections-0.1.15 local-attention-1.11.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 product-key-memory-0.2.11 reformer-pytorch-1.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install patool\n",
        "!pip install sktime\n",
        "! pip install reformer-pytorch==1.4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IeDSrzMxrND0",
        "outputId": "184e8190-e739-4252-9eb8-2f5572908dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Mar 31 06:01:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "326_2bpa7-LJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "ecd01bad-5677-44d5-aa29-1525c8b9cb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://6cdf-35-230-12-194.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app 'web'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:25] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:25] \"GET /static/back_img.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:26] \"GET /static/back_img2.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:27] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:28] \"\u001b[32mGET /work HTTP/1.1\u001b[0m\" 308 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:29] \"GET /work/ HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:30] \"GET /static/last_sample_prediction.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:35:30] \"GET /static/res_1.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:36:09] \"GET /static/res_1.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:36:12] \"GET /static/80.pdf HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据形状：(3480, 80, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:26] \"POST /work/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件已保存\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:42] \"GET /work/ HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:43] \"GET /static/last_sample_prediction.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:43] \"GET /static/res_1.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:44] \"GET /static/80.pdf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:44] \"GET /static/res_1.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:44] \"GET /static/80.pdf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2025 07:37:44] \"GET /static/res_1.jpg HTTP/1.1\" 200 -\n",
            "WARNING:pyngrok.process.ngrok:t=2025-05-10T07:38:35+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5000-2e667097-0c36-402b-988a-680f8ce52d61 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%run web.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TG_zH2FtpYAk",
        "outputId": "f6029d65-1eb1-4db4-e539-fc7d5f0342df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        0                   \n",
            "  Model ID:           ETTh1_96_96         Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          dataset/ETT-small   \n",
            "  Data Path:          test.csv            Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            16                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         24                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "loading model\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n",
            "=========\n",
            "3480\n",
            "=========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "3480\n",
            "=========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test shape: (3480, 80, 6) (3480, 80, 6)\n",
            "test shape: (3480, 80, 6) (3480, 80, 6)\n",
            "mse:0.8335068225860596, mae:0.4038733243942261, dtw:Not calculated, rmse:0.9129659533500671, r2:0.15639334917068481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
            "WARNING:matplotlib.font_manager:findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpO8_p3tuccX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/DLinear_ETTh1.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Oku51pRlgSK0",
        "outputId": "e58b6b2e-7098-49c0-a549-1e5dab418476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              DLinear             \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            4                   \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             1                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      1e-06               \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_DLinear_ETTh1_ftM_sl80_ll40_pl80_dm4_nh8_el2_dl1_df32_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 0.7114206\n",
            "\tspeed: 0.0153s/iter; left time: 24.3340s\n",
            "Epoch: 1 cost time: 1.8780913352966309\n",
            "Epoch: 1, Steps: 169 | Train Loss: 0.7840744 Vali Loss: 0.1136986 Test Loss: 0.1896977\n",
            "Validation loss decreased (inf --> 0.113699).  Saving model ...\n",
            "Updating learning rate to 1e-06\n",
            "\titers: 100, epoch: 2 | loss: 0.7979681\n",
            "\tspeed: 0.0247s/iter; left time: 35.1065s\n",
            "Epoch: 2 cost time: 1.194488286972046\n",
            "Epoch: 2, Steps: 169 | Train Loss: 0.7869116 Vali Loss: 0.1131233 Test Loss: 0.1889035\n",
            "Validation loss decreased (0.113699 --> 0.113123).  Saving model ...\n",
            "Updating learning rate to 5e-07\n",
            "\titers: 100, epoch: 3 | loss: 0.7908905\n",
            "\tspeed: 0.0207s/iter; left time: 25.8763s\n",
            "Epoch: 3 cost time: 1.059295415878296\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.7820108 Vali Loss: 0.1130929 Test Loss: 0.1885095\n",
            "Validation loss decreased (0.113123 --> 0.113093).  Saving model ...\n",
            "Updating learning rate to 2.5e-07\n",
            "\titers: 100, epoch: 4 | loss: 0.6223655\n",
            "\tspeed: 0.0191s/iter; left time: 20.6703s\n",
            "Epoch: 4 cost time: 1.0083537101745605\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.7775648 Vali Loss: 0.1129603 Test Loss: 0.1883142\n",
            "Validation loss decreased (0.113093 --> 0.112960).  Saving model ...\n",
            "Updating learning rate to 1.25e-07\n",
            "\titers: 100, epoch: 5 | loss: 0.7249010\n",
            "\tspeed: 0.0199s/iter; left time: 18.1670s\n",
            "Epoch: 5 cost time: 1.0588502883911133\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.7759955 Vali Loss: 0.1128944 Test Loss: 0.1882169\n",
            "Validation loss decreased (0.112960 --> 0.112894).  Saving model ...\n",
            "Updating learning rate to 6.25e-08\n",
            "\titers: 100, epoch: 6 | loss: 0.7782450\n",
            "\tspeed: 0.0196s/iter; left time: 14.6140s\n",
            "Epoch: 6 cost time: 1.048388957977295\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.7772308 Vali Loss: 0.1127739 Test Loss: 0.1881682\n",
            "Validation loss decreased (0.112894 --> 0.112774).  Saving model ...\n",
            "Updating learning rate to 3.125e-08\n",
            "\titers: 100, epoch: 7 | loss: 0.7226415\n",
            "\tspeed: 0.0201s/iter; left time: 11.5706s\n",
            "Epoch: 7 cost time: 1.0368380546569824\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.7778626 Vali Loss: 0.1127746 Test Loss: 0.1881438\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.5625e-08\n",
            "\titers: 100, epoch: 8 | loss: 1.2303983\n",
            "\tspeed: 0.0205s/iter; left time: 8.3819s\n",
            "Epoch: 8 cost time: 1.0946388244628906\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.7816335 Vali Loss: 0.1127828 Test Loss: 0.1881316\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 7.8125e-09\n",
            "\titers: 100, epoch: 9 | loss: 0.7295268\n",
            "\tspeed: 0.0207s/iter; left time: 4.9577s\n",
            "Epoch: 9 cost time: 1.0636398792266846\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.7765567 Vali Loss: 0.1128423 Test Loss: 0.1881254\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_DLinear_ETTh1_ftM_sl80_ll40_pl80_dm4_nh8_el2_dl1_df32_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.18925631046295166, mae:0.1974179595708847, rmse:0.43503597378730774, r2:0.3217695355415344, dtw:not calculated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/ETSformer_ETTh1.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pXZX-seQhD5m",
        "outputId": "15ac2550-af5f-447a-ba4d-351513539bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              ETSformer           \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            16                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           2                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            0                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use CPU\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_ETSformer_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl2_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 2.0493460\n",
            "\tspeed: 0.1535s/iter; left time: 244.2539s\n",
            "Epoch: 1 cost time: 25.034566164016724\n",
            "Epoch: 1, Steps: 169 | Train Loss: 2.5242538 Vali Loss: 0.6310313 Test Loss: 0.8237075\n",
            "Validation loss decreased (inf --> 0.631031).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 2.2706347\n",
            "\tspeed: 0.2941s/iter; left time: 418.2620s\n",
            "Epoch: 2 cost time: 24.67670178413391\n",
            "Epoch: 2, Steps: 169 | Train Loss: 1.9754674 Vali Loss: 0.4159947 Test Loss: 0.5823302\n",
            "Validation loss decreased (0.631031 --> 0.415995).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 1.4993382\n",
            "\tspeed: 0.2914s/iter; left time: 365.1844s\n",
            "Epoch: 3 cost time: 25.047765970230103\n",
            "Epoch: 3, Steps: 169 | Train Loss: 1.7219362 Vali Loss: 0.3513961 Test Loss: 0.5071293\n",
            "Validation loss decreased (0.415995 --> 0.351396).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 1.3738821\n",
            "\tspeed: 0.2960s/iter; left time: 320.9068s\n",
            "Epoch: 4 cost time: 25.46938705444336\n",
            "Epoch: 4, Steps: 169 | Train Loss: 1.6446461 Vali Loss: 0.3260371 Test Loss: 0.4765477\n",
            "Validation loss decreased (0.351396 --> 0.326037).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 1.6741585\n",
            "\tspeed: 0.2963s/iter; left time: 271.1428s\n",
            "Epoch: 5 cost time: 25.44494652748108\n",
            "Epoch: 5, Steps: 169 | Train Loss: 1.5619194 Vali Loss: 0.3147238 Test Loss: 0.4627327\n",
            "Validation loss decreased (0.326037 --> 0.314724).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 1.7148410\n",
            "\tspeed: 0.3013s/iter; left time: 224.7633s\n",
            "Epoch: 6 cost time: 25.314083576202393\n",
            "Epoch: 6, Steps: 169 | Train Loss: 1.5770881 Vali Loss: 0.3094785 Test Loss: 0.4558435\n",
            "Validation loss decreased (0.314724 --> 0.309479).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 1.5954539\n",
            "\tspeed: 0.2916s/iter; left time: 168.2808s\n",
            "Epoch: 7 cost time: 25.034103870391846\n",
            "Epoch: 7, Steps: 169 | Train Loss: 1.5748378 Vali Loss: 0.3064716 Test Loss: 0.4524343\n",
            "Validation loss decreased (0.309479 --> 0.306472).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 1.6034808\n",
            "\tspeed: 0.2973s/iter; left time: 121.3139s\n",
            "Epoch: 8 cost time: 24.524495124816895\n",
            "Epoch: 8, Steps: 169 | Train Loss: 1.5607163 Vali Loss: 0.3051897 Test Loss: 0.4506075\n",
            "Validation loss decreased (0.306472 --> 0.305190).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 1.4497427\n",
            "\tspeed: 0.2820s/iter; left time: 67.4069s\n",
            "Epoch: 9 cost time: 24.762106895446777\n",
            "Epoch: 9, Steps: 169 | Train Loss: 1.5461100 Vali Loss: 0.3042040 Test Loss: 0.4497099\n",
            "Validation loss decreased (0.305190 --> 0.304204).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 1.5546666\n",
            "\tspeed: 0.3017s/iter; left time: 21.1157s\n",
            "Epoch: 10 cost time: 26.18705987930298\n",
            "Epoch: 10, Steps: 169 | Train Loss: 1.5441920 Vali Loss: 0.3040774 Test Loss: 0.4492729\n",
            "Validation loss decreased (0.304204 --> 0.304077).  Saving model ...\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_ETSformer_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl2_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.45130330324172974, mae:0.45462387800216675, rmse:0.671791136264801, r2:-0.6173180341720581, dtw:not calculated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/LightTS_ETTh1.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bzswqq0HieI9",
        "outputId": "f6b47f19-2f12-48cb-cce7-b4c13d05353e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              LightTS             \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            16                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_LightTS_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 1.2911525\n",
            "\tspeed: 0.0196s/iter; left time: 31.2269s\n",
            "Epoch: 1 cost time: 2.554356098175049\n",
            "Epoch: 1, Steps: 169 | Train Loss: 1.3258214 Vali Loss: 0.3103392 Test Loss: 0.4138075\n",
            "Validation loss decreased (inf --> 0.310339).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.9107972\n",
            "\tspeed: 0.0290s/iter; left time: 41.3036s\n",
            "Epoch: 2 cost time: 1.599271297454834\n",
            "Epoch: 2, Steps: 169 | Train Loss: 1.0225055 Vali Loss: 0.2488095 Test Loss: 0.3317285\n",
            "Validation loss decreased (0.310339 --> 0.248809).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.7404224\n",
            "\tspeed: 0.0269s/iter; left time: 33.6603s\n",
            "Epoch: 3 cost time: 1.7381720542907715\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.8998436 Vali Loss: 0.2287055 Test Loss: 0.3054101\n",
            "Validation loss decreased (0.248809 --> 0.228705).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.7551796\n",
            "\tspeed: 0.0274s/iter; left time: 29.6964s\n",
            "Epoch: 4 cost time: 1.636601209640503\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.8551998 Vali Loss: 0.2198897 Test Loss: 0.2941107\n",
            "Validation loss decreased (0.228705 --> 0.219890).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.7386653\n",
            "\tspeed: 0.0264s/iter; left time: 24.1324s\n",
            "Epoch: 5 cost time: 1.6095712184906006\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.8368701 Vali Loss: 0.2156390 Test Loss: 0.2887494\n",
            "Validation loss decreased (0.219890 --> 0.215639).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.7839021\n",
            "\tspeed: 0.0258s/iter; left time: 19.2268s\n",
            "Epoch: 6 cost time: 1.5963571071624756\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.8288521 Vali Loss: 0.2134881 Test Loss: 0.2861178\n",
            "Validation loss decreased (0.215639 --> 0.213488).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.7343716\n",
            "\tspeed: 0.0253s/iter; left time: 14.6060s\n",
            "Epoch: 7 cost time: 1.5708427429199219\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.8254559 Vali Loss: 0.2125974 Test Loss: 0.2848017\n",
            "Validation loss decreased (0.213488 --> 0.212597).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.9282951\n",
            "\tspeed: 0.0277s/iter; left time: 11.3100s\n",
            "Epoch: 8 cost time: 1.7846777439117432\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.8258462 Vali Loss: 0.2120180 Test Loss: 0.2841396\n",
            "Validation loss decreased (0.212597 --> 0.212018).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.7451723\n",
            "\tspeed: 0.0259s/iter; left time: 6.1883s\n",
            "Epoch: 9 cost time: 1.6067681312561035\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.8192646 Vali Loss: 0.2119297 Test Loss: 0.2838033\n",
            "Validation loss decreased (0.212018 --> 0.211930).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.6923702\n",
            "\tspeed: 0.0255s/iter; left time: 1.7851s\n",
            "Epoch: 10 cost time: 1.5972487926483154\n",
            "Epoch: 10, Steps: 169 | Train Loss: 0.8185413 Vali Loss: 0.2117262 Test Loss: 0.2836332\n",
            "Validation loss decreased (0.211930 --> 0.211726).  Saving model ...\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_LightTS_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.28429707884788513, mae:0.3674245774745941, rmse:0.5331951379776001, r2:-0.018824219703674316, dtw:not calculated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vdyNog_6TdHR",
        "outputId": "03b7e10b-368f-41bb-98a7-7d49caf869e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              TSMixer             \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            16                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TSMixer_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 1.2464849\n",
            "\tspeed: 0.0198s/iter; left time: 31.4883s\n",
            "Epoch: 1 cost time: 2.4812371730804443\n",
            "Epoch: 1, Steps: 169 | Train Loss: 1.1598403 Vali Loss: 0.1613189 Test Loss: 0.2488521\n",
            "Validation loss decreased (inf --> 0.161319).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 1.1377280\n",
            "\tspeed: 0.0275s/iter; left time: 39.0584s\n",
            "Epoch: 2 cost time: 1.4740269184112549\n",
            "Epoch: 2, Steps: 169 | Train Loss: 0.8528322 Vali Loss: 0.1163889 Test Loss: 0.1854684\n",
            "Validation loss decreased (0.161319 --> 0.116389).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.6472571\n",
            "\tspeed: 0.0241s/iter; left time: 30.1798s\n",
            "Epoch: 3 cost time: 1.4300498962402344\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.7331024 Vali Loss: 0.1039112 Test Loss: 0.1685774\n",
            "Validation loss decreased (0.116389 --> 0.103911).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.8073524\n",
            "\tspeed: 0.0224s/iter; left time: 24.2831s\n",
            "Epoch: 4 cost time: 1.3917551040649414\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.6960690 Vali Loss: 0.0993861 Test Loss: 0.1622804\n",
            "Validation loss decreased (0.103911 --> 0.099386).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.9407721\n",
            "\tspeed: 0.0225s/iter; left time: 20.6024s\n",
            "Epoch: 5 cost time: 1.3532259464263916\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.6788287 Vali Loss: 0.0973038 Test Loss: 0.1595592\n",
            "Validation loss decreased (0.099386 --> 0.097304).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.5326695\n",
            "\tspeed: 0.0222s/iter; left time: 16.5939s\n",
            "Epoch: 6 cost time: 1.373352289199829\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.6671360 Vali Loss: 0.0962899 Test Loss: 0.1582517\n",
            "Validation loss decreased (0.097304 --> 0.096290).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.5526763\n",
            "\tspeed: 0.0226s/iter; left time: 13.0204s\n",
            "Epoch: 7 cost time: 1.4403789043426514\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.6659302 Vali Loss: 0.0957079 Test Loss: 0.1575958\n",
            "Validation loss decreased (0.096290 --> 0.095708).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.6192815\n",
            "\tspeed: 0.0243s/iter; left time: 9.9141s\n",
            "Epoch: 8 cost time: 1.5063495635986328\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.6636250 Vali Loss: 0.0955031 Test Loss: 0.1572653\n",
            "Validation loss decreased (0.095708 --> 0.095503).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.6420468\n",
            "\tspeed: 0.0233s/iter; left time: 5.5615s\n",
            "Epoch: 9 cost time: 1.3958191871643066\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.6621066 Vali Loss: 0.0953591 Test Loss: 0.1570970\n",
            "Validation loss decreased (0.095503 --> 0.095359).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.4861948\n",
            "\tspeed: 0.0222s/iter; left time: 1.5546s\n",
            "Epoch: 10 cost time: 1.3438541889190674\n",
            "Epoch: 10, Steps: 169 | Train Loss: 0.6610759 Vali Loss: 0.0954110 Test Loss: 0.1570096\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TSMixer_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.1578136831521988, mae:0.18463756144046783, rmse:0.3972577154636383, r2:0.4344492554664612, dtw:not calculated\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/TSMixer_ETTh1.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/TimeMixer_ETTh1.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RORzwODjjGnD",
        "outputId": "cd525a87-c972-4fa4-8889-068b11b38783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_80_80         Model:              TimeMixer           \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          0                   \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            4                   \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               16                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           10                  Learning Rate:      1e-06               \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_80_80_TimeMixer_ETTh1_ftM_sl80_ll0_pl80_dm4_nh8_el2_dl1_df16_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 1.3867309\n",
            "\tspeed: 0.0361s/iter; left time: 57.4352s\n",
            "Epoch: 1 cost time: 5.147141695022583\n",
            "Epoch: 1, Steps: 169 | Train Loss: 1.3725112 Vali Loss: 0.1861732 Test Loss: 0.3183806\n",
            "Validation loss decreased (inf --> 0.186173).  Saving model ...\n",
            "Updating learning rate to 1e-06\n",
            "\titers: 100, epoch: 2 | loss: 1.0958372\n",
            "\tspeed: 0.0572s/iter; left time: 81.2751s\n",
            "Epoch: 2 cost time: 3.9175853729248047\n",
            "Epoch: 2, Steps: 169 | Train Loss: 1.3482583 Vali Loss: 0.1835596 Test Loss: 0.3136269\n",
            "Validation loss decreased (0.186173 --> 0.183560).  Saving model ...\n",
            "Updating learning rate to 5e-07\n",
            "\titers: 100, epoch: 3 | loss: 1.5047587\n",
            "\tspeed: 0.0536s/iter; left time: 67.1255s\n",
            "Epoch: 3 cost time: 3.836015462875366\n",
            "Epoch: 3, Steps: 169 | Train Loss: 1.3316307 Vali Loss: 0.1822206 Test Loss: 0.3113224\n",
            "Validation loss decreased (0.183560 --> 0.182221).  Saving model ...\n",
            "Updating learning rate to 2.5e-07\n",
            "\titers: 100, epoch: 4 | loss: 1.2454656\n",
            "\tspeed: 0.0511s/iter; left time: 55.3669s\n",
            "Epoch: 4 cost time: 3.8537416458129883\n",
            "Epoch: 4, Steps: 169 | Train Loss: 1.3266244 Vali Loss: 0.1814656 Test Loss: 0.3101830\n",
            "Validation loss decreased (0.182221 --> 0.181466).  Saving model ...\n",
            "Updating learning rate to 1.25e-07\n",
            "\titers: 100, epoch: 5 | loss: 1.3395927\n",
            "\tspeed: 0.0550s/iter; left time: 50.3420s\n",
            "Epoch: 5 cost time: 4.061655521392822\n",
            "Epoch: 5, Steps: 169 | Train Loss: 1.3227780 Vali Loss: 0.1812708 Test Loss: 0.3096187\n",
            "Validation loss decreased (0.181466 --> 0.181271).  Saving model ...\n",
            "Updating learning rate to 6.25e-08\n",
            "\titers: 100, epoch: 6 | loss: 1.7767754\n",
            "\tspeed: 0.0515s/iter; left time: 38.3852s\n",
            "Epoch: 6 cost time: 3.859445571899414\n",
            "Epoch: 6, Steps: 169 | Train Loss: 1.3230390 Vali Loss: 0.1811641 Test Loss: 0.3093334\n",
            "Validation loss decreased (0.181271 --> 0.181164).  Saving model ...\n",
            "Updating learning rate to 3.125e-08\n",
            "\titers: 100, epoch: 7 | loss: 1.5472265\n",
            "\tspeed: 0.0525s/iter; left time: 30.2701s\n",
            "Epoch: 7 cost time: 3.9681105613708496\n",
            "Epoch: 7, Steps: 169 | Train Loss: 1.3200927 Vali Loss: 0.1806682 Test Loss: 0.3091927\n",
            "Validation loss decreased (0.181164 --> 0.180668).  Saving model ...\n",
            "Updating learning rate to 1.5625e-08\n",
            "\titers: 100, epoch: 8 | loss: 1.7890891\n",
            "\tspeed: 0.0522s/iter; left time: 21.2951s\n",
            "Epoch: 8 cost time: 3.8504486083984375\n",
            "Epoch: 8, Steps: 169 | Train Loss: 1.3161243 Vali Loss: 0.1807002 Test Loss: 0.3091224\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 7.8125e-09\n",
            "\titers: 100, epoch: 9 | loss: 1.5074437\n",
            "\tspeed: 0.0520s/iter; left time: 12.4169s\n",
            "Epoch: 9 cost time: 3.8838250637054443\n",
            "Epoch: 9, Steps: 169 | Train Loss: 1.3174679 Vali Loss: 0.1806785 Test Loss: 0.3090867\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Updating learning rate to 3.90625e-09\n",
            "\titers: 100, epoch: 10 | loss: 1.0240109\n",
            "\tspeed: 0.0526s/iter; left time: 3.6816s\n",
            "Epoch: 10 cost time: 3.978851795196533\n",
            "Epoch: 10, Steps: 169 | Train Loss: 1.3216036 Vali Loss: 0.1810280 Test Loss: 0.3090743\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Updating learning rate to 1.953125e-09\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_80_80_TimeMixer_ETTh1_ftM_sl80_ll0_pl80_dm4_nh8_el2_dl1_df16_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.31019556522369385, mae:0.20809847116470337, rmse:0.5569520592689514, r2:-0.11163568496704102, dtw:not calculated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/TimeXer_ETTh1.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DVkHdrbKjgka",
        "outputId": "7817ad7c-da02-4254-e072-703dbcfebe0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              TimeXer             \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            8                   \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            0                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use CPU\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimeXer_ETTh1_ftM_sl80_ll40_pl80_dm8_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 0.7133846\n",
            "\tspeed: 0.0573s/iter; left time: 91.0914s\n",
            "Epoch: 1 cost time: 9.224436283111572\n",
            "Epoch: 1, Steps: 169 | Train Loss: 0.9355427 Vali Loss: 0.1131574 Test Loss: 0.1938120\n",
            "Validation loss decreased (inf --> 0.113157).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.8570557\n",
            "\tspeed: 0.1057s/iter; left time: 150.3183s\n",
            "Epoch: 2 cost time: 8.239157915115356\n",
            "Epoch: 2, Steps: 169 | Train Loss: 0.8129643 Vali Loss: 0.1034665 Test Loss: 0.1776303\n",
            "Validation loss decreased (0.113157 --> 0.103466).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.5630605\n",
            "\tspeed: 0.1079s/iter; left time: 135.2153s\n",
            "Epoch: 3 cost time: 8.661869525909424\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.7696142 Vali Loss: 0.0998178 Test Loss: 0.1715686\n",
            "Validation loss decreased (0.103466 --> 0.099818).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.7549396\n",
            "\tspeed: 0.1085s/iter; left time: 117.6124s\n",
            "Epoch: 4 cost time: 8.80970311164856\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.7454948 Vali Loss: 0.0980001 Test Loss: 0.1686769\n",
            "Validation loss decreased (0.099818 --> 0.098000).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.7457221\n",
            "\tspeed: 0.1089s/iter; left time: 99.6252s\n",
            "Epoch: 5 cost time: 8.971843957901001\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.7386836 Vali Loss: 0.0972774 Test Loss: 0.1672630\n",
            "Validation loss decreased (0.098000 --> 0.097277).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.5640568\n",
            "\tspeed: 0.1052s/iter; left time: 78.4932s\n",
            "Epoch: 6 cost time: 8.692607879638672\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.7320974 Vali Loss: 0.0968422 Test Loss: 0.1665400\n",
            "Validation loss decreased (0.097277 --> 0.096842).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.6130765\n",
            "\tspeed: 0.1069s/iter; left time: 61.6618s\n",
            "Epoch: 7 cost time: 8.763477563858032\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.7315991 Vali Loss: 0.0966350 Test Loss: 0.1661834\n",
            "Validation loss decreased (0.096842 --> 0.096635).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.7615971\n",
            "\tspeed: 0.1085s/iter; left time: 44.2624s\n",
            "Epoch: 8 cost time: 8.541145324707031\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.7300137 Vali Loss: 0.0964563 Test Loss: 0.1660006\n",
            "Validation loss decreased (0.096635 --> 0.096456).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.6240455\n",
            "\tspeed: 0.1092s/iter; left time: 26.0889s\n",
            "Epoch: 9 cost time: 8.47775673866272\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.7325074 Vali Loss: 0.0963670 Test Loss: 0.1659108\n",
            "Validation loss decreased (0.096456 --> 0.096367).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.7132932\n",
            "\tspeed: 0.1082s/iter; left time: 7.5771s\n",
            "Epoch: 10 cost time: 8.696970224380493\n",
            "Epoch: 10, Steps: 169 | Train Loss: 0.7292184 Vali Loss: 0.0962169 Test Loss: 0.1658639\n",
            "Validation loss decreased (0.096367 --> 0.096217).  Saving model ...\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimeXer_ETTh1_ftM_sl80_ll40_pl80_dm8_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.16689743101596832, mae:0.1470489203929901, rmse:0.4085308313369751, r2:0.40189623832702637, dtw:not calculated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG4W35OdVxG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34ef10c-d2b3-48ff-faf5-b5b003ef2ac5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "Basic Config\n",
            "  Task Name:          long_term_forecast  Is Training:        1\n",
            "  Model ID:           ETTh1_96_96         Model:              TimesNet\n",
            "\n",
            "Data Loader\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv         Features:           M\n",
            "  Target:             OT                  Freq:               h\n",
            "  Checkpoints:        checkpoints/\n",
            "\n",
            "Forecasting Task\n",
            "  Seq Len:            80                  Label Len:          40\n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly\n",
            "  Inverse:            0\n",
            "\n",
            "Model Parameters\n",
            "  Top k:              5                   Num Kernels:        6\n",
            "  Enc In:             6                   Dec In:             6\n",
            "  C Out:              6                   d model:            16\n",
            "  n heads:            8                   e layers:           2\n",
            "  d layers:           1                   d FF:               32\n",
            "  Moving Avg:         25                  Factor:             3\n",
            "  Distil:             1                   Dropout:            0.1\n",
            "  Embed:              timeF               Activation:         gelu\n",
            "\n",
            "Run Parameters\n",
            "  Num Workers:        10                  Itr:                1\n",
            "  Train Epochs:       10                  Batch Size:         256\n",
            "  Patience:           3                   Learning Rate:      0.0001\n",
            "  Des:                Exp                 Loss:               MSE\n",
            "  Lradj:              type1               Use Amp:            0\n",
            "\n",
            "GPU\n",
            "  Use GPU:            1                   GPU:                0\n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3\n",
            "\n",
            "De-stationary Projector Params\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2\n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "        iters: 100, epoch: 1 | loss: 0.9783662\n",
            "        speed: 0.2424s/iter; left time: 385.6816s\n",
            "Epoch: 1 cost time: 38.94104313850403\n",
            "Epoch: 1, Steps: 169 | Train Loss: 0.8758022 Vali Loss: 0.1074075 Test Loss: 0.1837263\n",
            "Validation loss decreased (inf --> 0.107407).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "        iters: 100, epoch: 2 | loss: 0.7856071\n",
            "        speed: 0.4091s/iter; left time: 581.7415s\n",
            "Epoch: 2 cost time: 35.235536336898804\n",
            "Epoch: 2, Steps: 169 | Train Loss: 0.7677969 Vali Loss: 0.0948024 Test Loss: 0.1668558\n",
            "Validation loss decreased (0.107407 --> 0.094802).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "        iters: 100, epoch: 3 | loss: 0.8976045\n",
            "        speed: 0.4483s/iter; left time: 561.6709s\n",
            "Epoch: 3 cost time: 40.59457540512085\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.6728466 Vali Loss: 0.0872494 Test Loss: 0.1530264\n",
            "Validation loss decreased (0.094802 --> 0.087249).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "        iters: 100, epoch: 4 | loss: 0.7566913\n",
            "        speed: 0.4641s/iter; left time: 503.0939s\n",
            "Epoch: 4 cost time: 40.48423361778259\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.6185833 Vali Loss: 0.0849947 Test Loss: 0.1488827\n",
            "Validation loss decreased (0.087249 --> 0.084995).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "        iters: 100, epoch: 5 | loss: 0.6448200\n",
            "        speed: 0.4626s/iter; left time: 423.2469s\n",
            "Epoch: 5 cost time: 40.47894048690796\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.5972563 Vali Loss: 0.0845272 Test Loss: 0.1478218\n",
            "Validation loss decreased (0.084995 --> 0.084527).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "        iters: 100, epoch: 6 | loss: 0.7427856\n",
            "        speed: 0.4630s/iter; left time: 345.3705s\n",
            "Epoch: 6 cost time: 40.55626130104065\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.5966540 Vali Loss: 0.0804807 Test Loss: 0.1460695\n",
            "Validation loss decreased (0.084527 --> 0.080481).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "        iters: 100, epoch: 7 | loss: 0.3969599\n",
            "        speed: 0.4626s/iter; left time: 266.9373s\n",
            "Epoch: 7 cost time: 40.52998423576355\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.5921624 Vali Loss: 0.0794313 Test Loss: 0.1451459\n",
            "Validation loss decreased (0.080481 --> 0.079431).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "        iters: 100, epoch: 8 | loss: 0.4601535\n",
            "        speed: 0.4740s/iter; left time: 193.3939s\n",
            "Epoch: 8 cost time: 42.529244899749756\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.5918824 Vali Loss: 0.0792866 Test Loss: 0.1458095\n",
            "Validation loss decreased (0.079431 --> 0.079287).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "        iters: 100, epoch: 9 | loss: 0.7130401\n",
            "        speed: 0.4798s/iter; left time: 114.6810s\n",
            "Epoch: 9 cost time: 41.74711465835571\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.5905240 Vali Loss: 0.0790925 Test Loss: 0.1456372\n",
            "Validation loss decreased (0.079287 --> 0.079093).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "        iters: 100, epoch: 10 | loss: 0.6840795\n",
            "        speed: 0.4763s/iter; left time: 33.3403s\n",
            "Epoch: 10 cost time: 41.57422709465027\n",
            "Epoch: 10, Steps: 169 | Train Loss: 0.5916929 Vali Loss: 0.0790887 Test Loss: 0.1455483\n",
            "Validation loss decreased (0.079093 --> 0.079089).  Saving model ...\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.14617443084716797, mae:0.13625876605510712, dtw:Not calculated, rmse:0.3823276460170746, r2:0.4761604070663452\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/TimesNet_ETTh1_new.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/long_term_forecast/ETT_script/TimesNet_ETTh1_new.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8I3Tul40kVV",
        "outputId": "4fd7b01b-6ba6-4c65-d329-63d35a4e8468",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ETTh1_96_96         Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
            "  Data Path:          output.csv          Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        checkpoints/        \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            80                  Label Len:          40                  \n",
            "  Pred Len:           80                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             6                   Dec In:             6                   \n",
            "  C Out:              6                   d model:            16                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         256                 \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 43041\n",
            "val 11441\n",
            "test 8561\n",
            "\titers: 100, epoch: 1 | loss: 0.9779621\n",
            "\tspeed: 1.2629s/iter; left time: 2009.3224s\n",
            "Epoch: 1 cost time: 349.80813002586365\n",
            "Epoch: 1, Steps: 169 | Train Loss: 0.8763962 Vali Loss: 0.1072384 Test Loss: 0.1833779\n",
            "Validation loss decreased (inf --> 0.107238).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.7877297\n",
            "\tspeed: 5.6071s/iter; left time: 7973.3181s\n",
            "Epoch: 2 cost time: 505.3404710292816\n",
            "Epoch: 2, Steps: 169 | Train Loss: 0.7710030 Vali Loss: 0.0972537 Test Loss: 0.1692654\n",
            "Validation loss decreased (0.107238 --> 0.097254).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.9116367\n",
            "\tspeed: 4.7745s/iter; left time: 5982.4493s\n",
            "Epoch: 3 cost time: 446.23068952560425\n",
            "Epoch: 3, Steps: 169 | Train Loss: 0.6813218 Vali Loss: 0.0867973 Test Loss: 0.1535446\n",
            "Validation loss decreased (0.097254 --> 0.086797).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.7764407\n",
            "\tspeed: 4.6252s/iter; left time: 5013.7434s\n",
            "Epoch: 4 cost time: 447.55989623069763\n",
            "Epoch: 4, Steps: 169 | Train Loss: 0.6265228 Vali Loss: 0.0831435 Test Loss: 0.1499183\n",
            "Validation loss decreased (0.086797 --> 0.083143).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.6634115\n",
            "\tspeed: 4.8196s/iter; left time: 4409.8899s\n",
            "Epoch: 5 cost time: 462.5191614627838\n",
            "Epoch: 5, Steps: 169 | Train Loss: 0.6140311 Vali Loss: 0.0813399 Test Loss: 0.1480011\n",
            "Validation loss decreased (0.083143 --> 0.081340).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.7496047\n",
            "\tspeed: 4.8560s/iter; left time: 3622.5814s\n",
            "Epoch: 6 cost time: 466.56197357177734\n",
            "Epoch: 6, Steps: 169 | Train Loss: 0.6070371 Vali Loss: 0.0807284 Test Loss: 0.1478670\n",
            "Validation loss decreased (0.081340 --> 0.080728).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.4084683\n",
            "\tspeed: 4.9256s/iter; left time: 2842.0987s\n",
            "Epoch: 7 cost time: 471.64797258377075\n",
            "Epoch: 7, Steps: 169 | Train Loss: 0.6018769 Vali Loss: 0.0804684 Test Loss: 0.1477518\n",
            "Validation loss decreased (0.080728 --> 0.080468).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.4691186\n",
            "\tspeed: 4.9340s/iter; left time: 2013.0584s\n",
            "Epoch: 8 cost time: 471.931702375412\n",
            "Epoch: 8, Steps: 169 | Train Loss: 0.6011065 Vali Loss: 0.0805687 Test Loss: 0.1477487\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.7344710\n",
            "\tspeed: 4.9443s/iter; left time: 1181.6801s\n",
            "Epoch: 9 cost time: 473.02713227272034\n",
            "Epoch: 9, Steps: 169 | Train Loss: 0.6013179 Vali Loss: 0.0803588 Test Loss: 0.1476867\n",
            "Validation loss decreased (0.080468 --> 0.080359).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.6933970\n",
            "\tspeed: 4.9453s/iter; left time: 346.1677s\n",
            "Epoch: 10 cost time: 472.9250998497009\n",
            "Epoch: 10, Steps: 169 | Train Loss: 0.6023066 Vali Loss: 0.0803508 Test Loss: 0.1476013\n",
            "Validation loss decreased (0.080359 --> 0.080351).  Saving model ...\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl80_ll40_pl80_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 8561\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "test shape: (8561, 80, 6) (8561, 80, 6)\n",
            "mse:0.1483471691608429, mae:0.13807612657546997, rmse:0.38515862822532654, r2:0.4683740735054016, dtw:not calculated\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1g-KvkK-1lfKUJyJ67Esmk7Sjpa2htSnm",
      "authorship_tag": "ABX9TyMnUJYebzdmQQ+SGE7DOoxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}